# ğŸ§  NLP Pre-Processing Pipeline with Gradio

This project demonstrates a complete **Natural Language Processing (NLP) pre-processing pipeline** using **SpaCy**, **NLTK**, and **Gradio**.
It provides both a **text-based interface** (for single text input) and a **dataset-based interface** (for bulk processing from CSV/Excel files).

---

## ğŸš€ Try the App

Hugging Face Space: [NLP Pre-Processing Pipeline](https://huggingface.co/spaces/rayan2106/NLP-Preprocessing)

---

## ğŸš€ Features

- âœ… Run **individual NLP tasks** or a **full pipeline**
- âœ… Process **single text input** or **entire dataset columns**
- âœ… Choose between **stemming** or **lemmatization** in the full pipeline
- âœ… User-friendly **Gradio interface** with tabs for each function
- âœ… Supports `.csv` and `.xlsx` file formats

---

## ğŸ§© Pre-Processing Pipeline

Each NLP stage transforms the text in a specific way.
The available steps are:

| Step                               | Description                                                          |
| ---------------------------------- | -------------------------------------------------------------------- |
| **Tokenization**                   | Splits text into individual tokens (words, punctuation).             |
| **Stop Words**                     | Removes common words that carry less meaning (e.g., â€œtheâ€, â€œandâ€).   |
| **Stemming**                       | Reduces words to their root form using NLTKâ€™s `PorterStemmer`.       |
| **Lemmatization**                  | Converts words to their base form using SpaCy (context-aware).       |
| **POS Tagging**                    | Assigns parts of speech (noun, verb, adjective, etc.) to each token. |
| **NER (Named Entity Recognition)** | Detects named entities like people, locations, or organizations.     |

---

## âš™ï¸ Tech Stack

- **Python** â€“ main language
- **pandas** â€“ for dataset handling
- **NLTK** â€“ for stemming and stop-word filtering
- **SpaCy** â€“ for tokenization, lemmatization, POS, and NER
- **Gradio** â€“ for building the interactive interface

---

## ğŸ–¥ï¸ User Interface Overview

The app provides two main tabs:

### ğŸ”¹ 1. Text Tab

- Choose an NLP task (or full pipeline).
- Enter text directly into the textbox.
- See results instantly in structured JSON format.
- When running the **Full Pipeline**, select _stemming_ or _lemmatization_ mode.

### ğŸ”¹ 2. Dataset Tab

- Upload a `.csv` or `.xlsx` file.
- Select a column (or â€œAll Columnsâ€) to process.
- Choose which NLP function to apply.
- View processed results in JSON format.
- â€œFull Pipelineâ€ option also allows _stemming_ or _lemmatization_.

---

## â–¶ï¸ Run the Application

Clone repository and import the required libraries

```bash
python app.py
```

Once started, Gradio will launch in your browser (e.g., `http://127.0.0.1:7860`).

---

## ğŸ“ Example Use

**Text Mode:**

```text
Input: "Apple is looking at buying U.K. startup for $1 billion."
Output: JSON with tokens, POS tags, entities, and optional lemmatization or stemming.
```

**Dataset Mode:**

- Upload `reviews.csv`
- Select â€œreview_textâ€ column
- Choose â€œFull Pipelineâ€
- View processed tokens, entities, etc., for each row.

---

## ğŸ§± Example Output (Full Pipeline)

```json
{
  "tokenize": [
    "Apple",
    "is",
    "looking",
    "at",
    "buying",
    "U.K.",
    "startup",
    "for",
    "$",
    "1",
    "billion",
    "."
  ],
  "stop_words": ["Apple", "looking", "buying", "U.K.", "startup", "billion"],
  "pos_tagging": [
    ["Apple", "PROPN"],
    ["looking", "VERB"],
    ["startup", "NOUN"]
  ],
  "ner": [
    ["Apple", "ORG"],
    ["U.K.", "GPE"],
    ["$1 billion", "MONEY"]
  ],
  "lemmatized": [
    "Apple",
    "be",
    "look",
    "at",
    "buy",
    "U.K.",
    "startup",
    "for",
    "$",
    "1",
    "billion",
    "."
  ]
}
```

---

## ğŸ Summary

This project provides a **modular and interactive NLP pre-processing system** built with traditional, well-established libraries.
Itâ€™s ideal for **data cleaning, NLP model preparation, and educational demonstrations**.
